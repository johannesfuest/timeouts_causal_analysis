---
title: "Causal_analysis"
author: "Johannes Fuest"
date: "12/2/2023"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Read data and add correct defensive rating to df
```{r outcome regression}
df1 <- read.csv('data_with_to_info.csv')
names(df1)[names(df1) == "def_rating"] <- "def_rating_old"
names(df1)[names(df1) == "TimeBins"] <- "TimeBins_old"
df2 <- read.csv('poss_summary.csv')
df1 <- merge(df1, df2[c("GameID", "def_rating", "Possession", "TimeBins")], by = c("GameID", "Possession"), all.x = TRUE)
df1$def_rating_old <- NULL
df1$TimeBins_old <- NULL
#TO DO: unsure what this column is and why 33% of its values are null -> ask the squad
df1$HomeTeamTimeout <- NULL
df1 <- na.omit(df1)
# having both own and opp ptsperposs in causes multicollinearity (corr .99) -> keep only own
#keeping StartPossScoreDiff and StartTmScore and StartOppScore not possible due to multicollinearity -> keep only StartPossScorediff
#keeping period messes with regressions due to unseen levels in factor

columns_to_keep <- c("TimeBins","StartEvent","PossTeamWin","TimeoutCalledInitially",
                     "StartPossTime", "StartPossScoreDiff","StartTmPtsPerPoss","HomePoss","IsPlayoffs",
                     "SecondsSinceLastTimeout","ScoreDiffLastMinute", 
                     "off_rating", "def_rating", "TimeoutRemaining")

df1 <- df1[,columns_to_keep]
numeric_cols <- sapply(df1, is.numeric)
# df1[numeric_cols] <- scale(df1[numeric_cols])
df1 <- df1[!duplicated(df1), ]

# removing poss time <3
df1 <- df1[df1$StartPossTime>=3,]
df1$TimeBins <- cut(df1$StartPossTime, breaks = c(3,8,24), include.lowest = TRUE)
```
#TODO: decide on scaling

First up, we look at the basic diff-in-means:
```{r}
print(mean(df1[df1$TimeoutCalledInitially == TRUE, ]$PossTeamWin) - mean(df1[df1$TimeoutCalledInitially == FALSE, ]$PossTeamWin))
```
Calling a timeout appears to have a marginally positive effect, but there is 
only a .3% difference in means. It looks like not much is going on. 

Next, we split up by bins of seconds left, which is known to make an enormous 
difference and repeat our calculation.
#TODO: decide on time binning
```{r}
df_3to8 <- df1[df1$TimeBins == '[3,8]',]
df_8to24 <- df1[df1$TimeBins != '[3,8]',]
print(mean(df_3to8[df_3to8$TimeoutCalledInitially == TRUE, ]$PossTeamWin) - mean(df_3to8[df_3to8$TimeoutCalledInitially == FALSE, ]$PossTeamWin))
print(mean(df_8to24[df_8to24$TimeoutCalledInitially == TRUE, ]$PossTeamWin) - mean(df_8to24[df_8to24$TimeoutCalledInitially == FALSE, ]$PossTeamWin))

```
Here we can already see that the effect can change direction based on seconds left

Next, we split up by bins of seconds left, which is known to make an enormous 
difference and repeat our calculation.

```{r}
#resetting row names here so it doesn't mess with matching later
df_3to8_live <- df_3to8[df_3to8$StartEvent == "Live Ball Turnover",]
rownames(df_3to8_live)<- NULL
df_3to8_drb <- df_3to8[df_3to8$StartEvent == "Drb",]
rownames(df_3to8_drb)<- NULL
df_3to8_dead <- df_3to8[df_3to8$StartEvent != "Live Ball Turnover" & df_3to8$StartEvent != "Drb" ,]
rownames(df_3to8_dead)<- NULL
df_8to24_live <- df_8to24[df_8to24$StartEvent == "Live Ball Turnover",]
rownames(df_8to24_live)<- NULL
df_8to24_drb <- df_8to24[df_8to24$StartEvent == "Drb",]
rownames(df_8to24_drb)<- NULL
df_8to24_dead <- df_8to24[df_8to24$StartEvent != "Live Ball Turnover" & df_8to24$StartEvent != "Drb" ,]
rownames(df_8to24_dead)<- NULL

print(mean(df_3to8_live[df_3to8_live$TimeoutCalledInitially == TRUE, ]$PossTeamWin) - mean(df_3to8_live[df_3to8_live$TimeoutCalledInitially == FALSE, ]$PossTeamWin))
print(mean(df_3to8_drb[df_3to8_drb$TimeoutCalledInitially == TRUE, ]$PossTeamWin) - mean(df_3to8_drb[df_3to8_drb$TimeoutCalledInitially == FALSE, ]$PossTeamWin))
print(mean(df_3to8[df_3to8$TimeoutCalledInitially == TRUE, ]$PossTeamWin) - mean(df_3to8[df_3to8$TimeoutCalledInitially == FALSE, ]$PossTeamWin))

print(mean(df_8to24_live[df_8to24_live$TimeoutCalledInitially == TRUE, ]$PossTeamWin) - mean(df_8to24_live[df_8to24_live$TimeoutCalledInitially == FALSE, ]$PossTeamWin))
print(mean(df_8to24_drb[df_8to24_drb$TimeoutCalledInitially == TRUE, ]$PossTeamWin) - mean(df_8to24_drb[df_8to24_drb$TimeoutCalledInitially == FALSE, ]$PossTeamWin))
print(mean(df_8to24_dead[df_8to24_dead$TimeoutCalledInitially == TRUE, ]$PossTeamWin) - mean(df_8to24_dead[df_8to24_dead$TimeoutCalledInitially == FALSE, ]$PossTeamWin))
```
With the exception of possessions with more than 12 seconds remaining, we see that
the sign of diff-in-means changes depending on event. With these meta_variables 
in place, we can now begin estimating the true treatment effect of the timeout
call holding constant our covariates. We will do this by using AIPW:

1.) Esimate propensity scores using logistic regression. We will keep meta
variables in model. #TODO: ask Dominik/TAs if this is fine (bias?) We also do not
use Timeoutremaining here
```{r}
# Use only TimeoutRemaining == TRUE data for propensity score estimation
df_timeouts <- df1[df1$TimeoutRemaining == TRUE,]
# not using starting scores and per minute scores to avoid multicollinearity (was getting N/A coef values)
# e_x <- glm(TimeoutCalledInitially ~ 1+StartEvent+StartPossTime+StartPossScoreDiff
#            +StartTmPtsPerPoss+HomePoss+IsPlayoffs+SecondsSinceLastTimeout
#            +ScoreDiffLastMinute+off_rating+def_rating,
#              data = df_timeouts, family = binomial)
# summary(e_x)
```
2.) Estimate outcome variable with logistic regression for each df
```{r}
# missing
df_list <- list(df_3to8_live,df_3to8_drb, df_3to8_dead, df_8to24_live, df_8to24_drb, df_8to24_dead)
model_list <- list()
e_x_list <- list()

# set up cv for RF
  seeds <- vector("list", length = 6)
for (j in 1:5) {
  seeds[[j]] <- c(1,2,3)
}
seeds[[6]] <- c(1,2,3,4,5)
  
  # rfControl <- trainControl(method = "cv", number = 5, seeds=seeds)
 rfControl <- trainControl(method = "none")

# Loop through each DataFrame and fit a logistic regression model on treatments and controls respectively (have to leave out starting event, also leaving out period due to issues from rare overtimes)
for (i in 1:length(df_list)) {
  model_1 <-  train(as.factor(PossTeamWin) ~ 1+StartPossTime+StartPossScoreDiff+StartTmPtsPerPoss
                 +HomePoss+IsPlayoffs+SecondsSinceLastTimeout+ScoreDiffLastMinute
                 +off_rating+def_rating, df_list[[i]][df_list[[i]]$TimeoutCalledInitially==TRUE,], family = binomial, method = "rf", trControl = rfControl)
  
  model_0 <-  train(as.factor(PossTeamWin) ~ 1+StartPossTime+StartPossScoreDiff+StartTmPtsPerPoss
                 +HomePoss+IsPlayoffs+SecondsSinceLastTimeout+ScoreDiffLastMinute
                 +off_rating+def_rating, df_list[[i]][df_list[[i]]$TimeoutCalledInitially==FALSE,], family = binomial, method = "rf", trControl = rfControl)
  
e_x <- glm(TimeoutCalledInitially ~ 1+StartPossTime+StartPossScoreDiff
           +StartTmPtsPerPoss+HomePoss+IsPlayoffs+SecondsSinceLastTimeout
           +ScoreDiffLastMinute+off_rating+def_rating,
             data = df_list[[i]][df_list[[i]]$TimeoutRemaining==TRUE,], family = binomial)
  
  model_list[[i]] <- list(model_1, model_0)
  e_x_list[[i]] <- e_x
}
```
3.) Estimate $$\hat{\mu}^{adj}$$
```{r}
tau_aipws <- list()
for (i in 1:length(df_list)) {
  df_temp <- df_list[[i]]
  mu_hat_1 <- mean((df_temp$TimeoutCalledInitially * (df_temp$PossTeamWin - (predict(model_list[[i]][[1]], newdata=df_temp, type='prob')[,2]))/predict(e_x_list[[i]], df_temp, type = "response")) +  (predict(model_list[[i]][[1]], newdata=df_temp, type='prob')[,2]))
  mu_hat_0 <- mean(((1-df_temp$TimeoutCalledInitially) * (df_temp$PossTeamWin - (predict(model_list[[i]][[2]], newdata=df_temp, type='prob')[,2]))/(1-predict(e_x_list[[i]], df_temp, type = "response"))) + (predict(model_list[[i]][[2]], newdata=df_temp, type='prob')[,2]))
  tau_aipws[[i]] <- (mu_hat_1 - mu_hat_0)
}
print(tau_aipws)
```
Next, we use the bootstrap to estimate the variance of our AIPW taus
Question: how to bootstrap when model must be re-estimated each time?
```{r}
tau_boot1 <- list()
tau_boot2 <- list()
tau_boot3 <- list()
tau_boot4 <- list()
tau_boot5 <- list()
tau_boot6 <- list()
tau_boot <- list(tau_boot1, tau_boot2, tau_boot3, tau_boot4, tau_boot5, tau_boot6)
for (j in 1:1200){
    bootstrapped_df <- df_list[[(j%%6) + 1]][sample(nrow(df_list[[(j%%6) + 1]]), nrow(df_list[[(j%%6) + 1]]), replace = TRUE), ]
    e_x <- glm(TimeoutCalledInitially ~ 1+StartPossTime+StartPossScoreDiff
           +StartTmPtsPerPoss+HomePoss+IsPlayoffs+SecondsSinceLastTimeout
           +ScoreDiffLastMinute+off_rating+def_rating,
             data = bootstrapped_df[bootstrapped_df$TimeoutRemaining==TRUE,], family = binomial)
    
      model_1 <-  train(as.factor(PossTeamWin) ~ 1+StartPossTime+StartPossScoreDiff+StartTmPtsPerPoss
                 +HomePoss+IsPlayoffs+SecondsSinceLastTimeout+ScoreDiffLastMinute
                 +off_rating+def_rating,bootstrapped_df[bootstrapped_df$TimeoutCalledInitially==TRUE,], family = binomial, method = "rf", trControl = rfControl)
  
  model_0 <-  train(as.factor(PossTeamWin) ~ 1+StartPossTime+StartPossScoreDiff+StartTmPtsPerPoss
                 +HomePoss+IsPlayoffs+SecondsSinceLastTimeout+ScoreDiffLastMinute
                 +off_rating+def_rating, bootstrapped_df[bootstrapped_df$TimeoutCalledInitially==FALSE,], family = binomial, method = "rf", trControl = rfControl)
    
    mu_hat_1 <- mean((bootstrapped_df$TimeoutCalledInitially * (bootstrapped_df$PossTeamWin -predict(model_1, newdata=bootstrapped_df, type='prob')[,2])/predict(e_x, bootstrapped_df, type = "response")) + predict(model_1, newdata=bootstrapped_df, type='prob')[,2])
    
    mu_hat_0 <- mean(((1-bootstrapped_df$TimeoutCalledInitially) * (bootstrapped_df$PossTeamWin -predict(model_0, newdata=bootstrapped_df, type='prob')[,2])/(1-predict(e_x, bootstrapped_df, type = "response"))) + predict(model_0, newdata=bootstrapped_df, type='prob')[,2])
    tau_boot[[(j%%6) + 1]][[(as.integer(j/6)%%200) + 1]] <- (mu_hat_1 - mu_hat_0)
}
```
This gives us CIs for all of our taus for AIPW:
```{r}
for (i in 1:6) {
  alpha <- 0.05
  lower_bound <- sort(unlist(tau_boot[[i]]))[[5]]
  upper_bound <- sort(unlist(tau_boot[[i]]))[[195]]
  conf_interval <- c(lower_bound, upper_bound)
  print(conf_interval)
}

```
Some of our taus are not zero. More analysis later.

Next: matched pairs design:
```{r}
library(DOS2)
library(RItools)
library(optmatch)
library(plyr)
library(rcbalance)
```

```{r}

for(i in 1:6){
  df_temp <- df_list[[i]]
  df1$prop <- predict(e_x, df1, type = "response")
ggplot(data=df_temp, aes(x=prop, group=as.factor(TimeoutCalledInitially),
fill=as.factor(TimeoutCalledInitially))) + geom_density(alpha=0.5) + theme_bw()
  mat.1 <- smahal(df_temp$TimeoutCalledInitially, df_temp[,c("StartPossTime","StartPossScoreDiff","StartTmPtsPerPoss","HomePoss","IsPlayoffs","SecondsSinceLastTimeout","ScoreDiffLastMinute","off_rating","def_rating")])
ms.2 <- pairmatch(mat.1, data=df_temp)
plot(xBalance(TimeoutCalledInitially~StartPossTime+StartPossScoreDiff+StartTmPtsPerPoss
                 +HomePoss+IsPlayoffs+SecondsSinceLastTimeout+ScoreDiffLastMinute
                 +off_rating+def_rating,
strata=list(unstrat=NULL, ms.2=~ms.2),
data=df_temp))
legend(
    "topright",
    legend = c("Pre matching", "Post Matching"),
    inset = 0.01,
    pch = c(15, 16),
    bg = "white"
)
# summa <- summarize.Match(df_temp, ms.2)
# TODO: fix matching 
}
```
IV regression using TimeoutRemaining
as an instrumental variable:
```{r}
print(cor(df1$TimeoutRemaining, df1$TimeoutCalledInitially))
```
Correlation between our instrument and treatment are high. 

```{r}
library(AER)
regs <- list()
for (i in 1:6) {
  result <- ivreg(PossTeamWin ~ TimeoutCalledInitially+StartPossTime+StartPossScoreDiff+StartTmPtsPerPoss
                 +HomePoss+IsPlayoffs+SecondsSinceLastTimeout+ScoreDiffLastMinute
                 +off_rating+def_rating|TimeoutRemaining+StartPossTime+StartPossScoreDiff+StartTmPtsPerPoss
                 +HomePoss+IsPlayoffs+SecondsSinceLastTimeout+ScoreDiffLastMinute
                 +off_rating+def_rating, data = df_list[[i]])
  regs[[i]] <- summary(result)
  
}

for (i in 1:6) {
  reg <- regs[[i]]
  alpha <- 0.05
  lower_bound <- coef(reg)[2] - 1.96*reg$coefficients[2, "Std. Error"]
  upper_bound <- coef(reg)[2] + 1.96*reg$coefficients[2, "Std. Error"]
  conf_interval <- c(lower_bound, upper_bound)
  print(conf_interval)
}
```
These numbers are nonsensical, need to do IVreg with logistic regressions (2SRI):
#TODO: discuss 2SRI with Dominic
```{r}
tau_ivs <- numeric(6)
for (i in 1:6) {
  #1. perform log regression of timeoutcalled on covariates and instrument
  # model_1 <- glm(TimeoutCalledInitially ~ StartPossTime+StartPossScoreDiff+StartTmPtsPerPoss +HomePoss+IsPlayoffs+ScoreDiffLastMinute+off_rating+def_rating, data = df_list[[i]], family = "binomial")
  df_temp <- df_list[[i]]

# Standardize only numeric predictors
df_numeric_scaled <- scale(df_temp[, c("StartPossTime", "StartPossScoreDiff", "StartTmPtsPerPoss", 
                      "HomePoss", "IsPlayoffs", "SecondsSinceLastTimeout", "ScoreDiffLastMinute", 
                      "off_rating", "def_rating")])

# Combine the scaled numeric data with the boolean data
X <- as.matrix(cbind(df_numeric_scaled, df_temp[, "TimeoutRemaining"]))

# Prepare the response variable
y <- df_temp$TimeoutCalledInitially

# Fit the Ridge logistic regression model
cv_fit <- cv.glmnet(X, y, alpha=0, family="binomial")
  
  #2. generate predictions of timeoutcalled. 
  # df_list[[i]]$timeoutresids <- df_list[[i]]$TimeoutCalledInitially - predict(model_1, df_list[[i]], type = "response")
  model_1 <- glmnet(X, y, alpha=0, lambda=cv_fit$lambda.min, family="binomial")
  df_temp$timeoutresids <- df_temp$TimeoutCalledInitially - predict(model_1, X, type = "response")
  #3. Perform second log. regression of TeamPossWon on predicted Timeout and covariates
  # model_2 <- glm(PossTeamWin ~ TimeoutCalledInitially+timeoutresids+StartPossTime+StartPossScoreDiff+StartTmPtsPerPoss
  #                +HomePoss+IsPlayoffs+SecondsSinceLastTimeout+ScoreDiffLastMinute
  #                +off_rating+def_rating,
  #              data = df_list[[i]], family =binomial(link="logit"))
  y <- df_temp$PossTeamWin
  df_numeric_scaled <- scale(df_temp[, c("TimeoutCalledInitially", "timeoutresids", "StartPossTime", "StartPossScoreDiff", "StartTmPtsPerPoss", 
                      "HomePoss", "IsPlayoffs", "SecondsSinceLastTimeout", "ScoreDiffLastMinute", 
                      "off_rating", "def_rating")])
  X <- as.matrix(cbind(df_numeric_scaled, df_temp[, "TimeoutRemaining"]))
  # Fit the Ridge logistic regression model
  cv_fit <- cv.glmnet(X, y, alpha=0, family="binomial")
  model_2 <- glmnet(X, y, alpha=0, lambda=cv_fit$lambda.min, family="binomial")

  # Combine the scaled numeric data with the boolean data
  df_temp[, "TimeoutRemaining"] <- TRUE
  X <- as.matrix(cbind(df_numeric_scaled, df_temp[, "TimeoutRemaining"]))
  
  # exogeneity test
  # print("exogeneity")
  # print(cor(resid(glm(PossTeamWin ~ TimeoutCalledInitially+StartPossTime+StartPossScoreDiff+StartTmPtsPerPoss
  #                +HomePoss+IsPlayoffs+SecondsSinceLastTimeout+ScoreDiffLastMinute
  #                +off_rating+def_rating,
  #              data = df_list[[i]], family =binomial(link="logit"))), df_list[[i]]$TimeoutRemaining))
  # relevance test
  # print("relevance: ")
  # print(cor(df_list[[i]]$TimeoutRemaining, df_list[[i]]$TimeoutCalledInitially))
  #4. calculate the difference in predicted probabilities of the outcome with and without treatment and average. 
  # df_temp$TimeoutCalledInitially <- TRUE
  preds_1 <- predict(model_2, X, type = "response")
  
  df_temp[, "TimeoutRemaining"] <- FALSE
  X <- as.matrix(cbind(df_numeric_scaled, df_temp[, "TimeoutRemaining"]))
  preds_0 <- predict(model_2, X, type = "response")
  tau_ivs[i] <- mean(preds_1 - preds_0)
}
print(tau_ivs)

ggplot(df1, aes(x = StartEvent, y = TimeoutCalledInitially)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(TimeoutCalledInitially, 3)), vjust = -0.3) + # Adding text above bars
  labs(y = "Timeout Rate", x = "Start Event") +
  theme_minimal()
```
Some of these numbers are suspiciously large. But this could point to the fact
that our analysis is dampened by many unobserved confounders

#TODO: derive CIs for this

